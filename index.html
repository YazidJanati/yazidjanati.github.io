<html>

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Janati, Yazid">
    <link href="style.css" rel="stylesheet" type="text/css" />
    <title>Yazid Janati El Idrissi</title>
    <meta name="google-site-verification" content="IlLYk-Yxb3YHzeJP5D02V6L1MIW1nrxbbUwThwXJfNQ" />
</head>

<body>
    <div id="title"></div>

    <div class="main" style="padding-right: 2px; 
                  margin-left: 200px; 
                  margin-right: 200px;
                  padding-left: 2px;"><!-- <div style="float: left; width: 100%"> -->

        <div style="width:100%">
            <p><span class="boldbig">Yazid Janati</span></p>
        </div>
        <a href="https://scholar.google.com/citations?user=JGor6XwAAAAJ&hl=en"></br>Google Scholar</a>
        <a href="https://github.com/yazidjanati"> / Github</a>
        <a href="mailto: janati.yazid@gmail.com"> / Email</a>


        <p></br>
            I am currently a postdoctoral research at CMAP, Ecole polytechnique working with Eric Moulines and <a
                href="https://alain.perso.math.cnrs.fr">Alain Durmus</a>. Previously I was a PhD candidate in Statistics
            at Télécom SudParis - Institut Polytechnique de Paris, advised by <a
                href="https://sylvainlc.github.io">Sylvain Le Corff</a> (LPSM, Sorbonne Université) and <a
                href="http://www-public.imtbs-tsp.eu/~petetin/">Yohan Petetin</a> (CITI, Télécom SudParis).
            </br></br>
            My PhD work focused on building new algorithms related to Monte Carlo methods and studying their theoretical
            properties. I was and still am particularly interested in the interplay between MC and deep learning
            methods. At the moment I work on solving inverse problems with Denoising Diffusion models.
            </br>
            </br>
            <hr>
        <p><span class="boldbig">Research</span></p>

        <p><b>Bridging Diffusion Posterior Sampling and Monte Carlo methods: a survey</b></a>
            </br>
            Y. Janati, A. Durmus, J. Olsson, E. Moulines
            </br><em>To appear at Philosophical Transactions A of the Royal Society, special issue on Bayesian inverse
                problems with generative models.
            </em>
            </br>
        <p><a href="https://arxiv.org/abs/2502.03332"><b>A Mixture-Based Framework for Guiding Diffusion Models</b></a>
            </br>
            Y. Janati, B. Moufad, M. Abou El Qassime, A. Durmus, E. Moulines, J. Olsson.
            </br><em>Under review.</em>
            </br>
        <p><a href="https://arxiv.org/abs/2410.09945"><b>Variational Diffusion Posterior Sampling with Midpoint
                    Guidance</b></a>
            </br>
            B. Moufad, Y. Janati, L. Bedin, A. Durmus, R. Douc, E. Moulines, J. Olsson.
            </br><em>International Conference on Learning Representations (ICLR). 2025.</em> <b>Oral, top 1.8%.</b>
            </br>

        <p><a href="https://arxiv.org/abs/2403.11407"><b>Divide-and-Conquer posterior sampling with Denoising Diffusion
                    priors</b></a>
            </br>
            Y. Janati, B. Moufad, A. Durmus, E. Moulines, J. Olsson.
            </br><em>Advances in Neural Information Processing Systems (NeurIPS). 2024.</em>
            </br>

            <!-- <p><a href="https://arxiv.org/abs/2308.07983"><b>Entropic Mirror Monte Carlo</b></a>
            </br>
            Y. Janati, A. Durmus, S. Le Corff, Y. Petetin, J. Stoehr.
            </br><em>Under review.</em>
            </br> -->

        <p><a href="https://openreview.net/pdf?id=nHESwXvxWK"><b>Monte Carlo guided Denoising Diffusion models for
                    Bayesian linear inverse
                    problems</b></a>
            </br>
            G. Cardoso, Y. Janati, S. Le Corff, E. Moulines.
            </br><em>International Conference on Learning Representations (ICLR). 2024.</em> <b>Oral, top 1.2%.</b>
            </br>

        <p><a href="https://arxiv.org/abs/2301.00900"><b>State and parameter learning with PaRISian Particle
                    Gibbs</b></a>
            </br>
            G. Cardoso, Y. Janati, S. Le Corff, E. Moulines and J. Olsson.
            </br><em>International Conference on Machine Learning (ICML). 2023.</em>
            </br>

        <p><a href="https://arxiv.org/pdf/2204.01401.pdf"><b>Variance estimation for Sequential Monte Carlo Algorithms:
                    a backward sampling approach
                </b></a>
            </br>
            Y. Janati, S. Le Corff and Y. Petetin.
            </br><em>Bernoulli. 2024.</em>
            </br>

        <p><a href="https://proceedings.neurips.cc/paper/2021/file/8dd291cbea8f231982db0fb1716dfc55-Paper.pdf"><b>NEO:
                    Non Equilibrium Sampling on the Orbit of a Deterministic Transform</b></a>
            </br>
            A. Thin, Y. Janati, S. Le Corff, C. Ollion, A. Doucet, A. Durmus, E. Moulines and C. Robert.
            </br><em>Advances in Neural Information Processing Systems (NeurIPS). 2021.</em>
            </br>

        <p><a href="https://ieeexplore.ieee.org/document/9540246/"><b>Structured variational Bayesian inference for
                    Gaussian state-space models with regime switching.</b></a>
            </br>
            Y. Petetin, Y. Janati and F. Desbouvries.
            </br><em>IEEE Signal Processing Letters 28. 2021.</em>
            </br>
        </p>
    </div>
</body>

</html>