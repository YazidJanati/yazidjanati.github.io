<html>

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Janati, Yazid">
    <link href="style.css" rel="stylesheet" type="text/css" />
    <title>Yazid Janati El Idrissi</title>
    <meta name="google-site-verification" content="IlLYk-Yxb3YHzeJP5D02V6L1MIW1nrxbbUwThwXJfNQ" />
</head>

<body>
    <div id="title"></div>

    <div class="main" style="padding-right: 2px; 
                  margin-left: 200px; 
                  margin-right: 200px;
                  padding-left: 2px;"><!-- <div style="float: left; width: 100%"> -->

        <div style="width:100%">
            <p><span class="boldbig">Yazid Janati</span></p>
        </div>
        <a href="https://scholar.google.com/citations?user=JGor6XwAAAAJ&hl=en"></br>Google Scholar</a>
        <a href="https://github.com/yazidjanati"> / Github</a>
        <a href="mailto: janati.yazid@gmail.com"> / Email</a>


        <p></br>
            I am currently a postdoctoral research at CMAP, Ecole polytechnique working with Eric Moulines and <a
                href="https://alain.perso.math.cnrs.fr">Alain Durmus</a>. Previously I was a PhD candidate in Statistics
            at Télécom SudParis - Institut Polytechnique de Paris, advised by <a
                href="https://sylvainlc.github.io">Sylvain Le Corff</a> (LPSM, Sorbonne Université) and <a
                href="http://www-public.imtbs-tsp.eu/~petetin/">Yohan Petetin</a> (CITI, Télécom SudParis).
            </br></br>
            My PhD work focused on building new algorithms related to Monte Carlo methods and studying their theoretical
            properties. I was and still am particularly interested in the interplay between MC and deep learning
            methods. At the moment I work on solving inverse problems with Denoising Diffusion models.
            </br>
            </br>
            <hr>
        <p><span class="boldbig">Research</span></p>

        <p><a href="https://arxiv.org/abs/2403.11407"><b>Divide-and-Conquer posterior sampling with Denoising Diffusion
                    priors</b></a>
            </br>
            YJ, B. Moufad, A. Durmus, E. Moulines, J. Olsson.
            </br><em>Advances in Neural Information Processing Systems (NeurIPS) 38 (2024).</em>
            </br>

        <p><a href="https://arxiv.org/abs/2308.07983"><b>Entropic Mirror Monte Carlo</b></a>
            </br>
            YJ, A. Durmus, S. Le Corff, Y. Petetin, J. Stoehr.
            </br><em>Under review.</em>
            </br>

        <p><a href="https://arxiv.org/abs/2308.07983"><b>Monte Carlo guided Diffusion for Bayesian linear inverse
                    problems</b></a>
            </br>
            G. Cardoso, YJ, S. Le Corff, E. Moulines.
            </br><em>International Conference on Learning Representations (ICLR) 2024.</em> <b>Oral, top 1.2%.</b>
            </br>

        <p><a href="https://arxiv.org/abs/2301.00900"><b>State and parameter learning with PaRISian Particle
                    Gibbs</b></a>
            </br>
            G. Cardoso, YJ, S. Le Corff, E. Moulines and J. Olsson.
            </br><em>International Conference on Machine Learning (ICML) 2023.</em>
            </br>

        <p><a href="https://arxiv.org/pdf/2204.01401.pdf"><b>Variance estimation for Sequential Monte Carlo Algorithms:
                    a backward sampling approach
                </b></a>
            </br>
            YJ, S. Le Corff and Y. Petetin.
            </br><em>Bernoulli 30 (2), 911-935.</em>
            </br>

        <p><a href="https://proceedings.neurips.cc/paper/2021/file/8dd291cbea8f231982db0fb1716dfc55-Paper.pdf"><b>NEO:
                    Non Equilibrium Sampling on the Orbit of a Deterministic Transform</b></a>
            </br>
            A. Thin, YJ, S. Le Corff, C. Ollion, A. Doucet, A. Durmus, E. Moulines and C. Robert.
            </br><em>Advances in Neural Information Processing Systems (NeurIPS) 34 (2021): 17060-17071.</em>
            </br>

        <p><a href="https://ieeexplore.ieee.org/document/9540246/"><b>Structured variational Bayesian inference for
                    Gaussian state-space models with regime switching.</b></a>
            </br>
            Y. Petetin, YJ and F. Desbouvries.
            </br><em>IEEE Signal Processing Letters 28 (2021): 1953-1957.</em>
            </br>

            </br>
        <p><span class="boldbig">Talks and posters</span></p>
        <b>Monte Carlo guided Diffusion for Bayesian linear inverse problems</b>
        <ul>
            <li>Mostly Monte Carlo Seminar, Paris Santé Campus, 11/2023</li>
            <li>Un-conference Hawaii in Paris, HEC Paris, 07/2023.</li>
        </ul>
        </br>
        <b>An iterative scheme for backward Kullback-Leibler minimization</b>
        <ul>
            <li>Journée de la statistique, Université libre de Bruxelles, 07/2023</li>
            <li>PhD statistics seminar, University of Edinburgh (online), 02/2023</li>
            <li>Workshop "On Future Trends and Opportunities for Monte Carlo methods", University of Warsaw, 12/2022.
            </li>
            <li>PhD students seminar, LPSM, Sorbonne Université, 11/2022.</li>
            <li>Rencontres statistiques du CEREMADE, Université Paris Dauphine, 11/2022.</li>
            <li>MIA seminar, Agro ParisTech, 09/2022.</li>
            <li>Mathematics in Machine Learning summer school, UM6P University, Ben Guerir, 07/2022.</li>
        </ul>
        </br>
        <b>Variance estimation for Sequential Monte Carlo Algorithms</b>
        <ul>
            <li>5th SMC workshop (poster), UC3M, Madrid, 05/2022.</li>
            <li>PhD students seminar, Télécom Paris, 04/2022.</li>
        </ul>
        </br>
        <b>Non Equilibrium Sampling on the Orbit of a Deterministic Transform</b>
        <ul>
            <li>NeuRIPS in Paris (poster), SCAI Sorbonne, 11/2021.</li>
        </ul>
        </p>
    </div>
</body>

</html>